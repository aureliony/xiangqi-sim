{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu') # for easier debugging\n",
    "RANDOM_SEED = 42\n",
    "RANDOM_GENERATOR = torch.Generator().manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.io.image import ImageReadMode\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "BLACK_PIECES = \"将士象車馬砲卒\"\n",
    "RED_PIECES = \"帅仕相俥傌炮兵\"\n",
    "VOCAB = BLACK_PIECES + RED_PIECES\n",
    "NUM_CLASSES = len(VOCAB)\n",
    "\n",
    "PADDING_VALUE = NUM_CLASSES\n",
    "# IGNORE_INDEX = -100 # Specify to ignore certain targets during loss computation\n",
    "\n",
    "MAX_LABEL_LEN = 1\n",
    "\n",
    "\n",
    "def get_image_paths(folder):# -> list:\n",
    "    return [os.path.join(folder, img) for img in os.listdir(folder)]\n",
    "\n",
    "\n",
    "def get_filenames(paths):\n",
    "    return [os.path.basename(path).split('.')[0] for path in paths]\n",
    "\n",
    "\n",
    "def label_to_integers(label):\n",
    "    assert MAX_LABEL_LEN is not None\n",
    "    integers = [VOCAB.index(c) for c in label]\n",
    "    assert len(integers) <= MAX_LABEL_LEN\n",
    "    integers.extend([PADDING_VALUE] * (MAX_LABEL_LEN - len(integers))) # Add padding\n",
    "    return integers\n",
    "\n",
    "\n",
    "def paths_to_labels(paths):\n",
    "    filenames = get_filenames(paths)\n",
    "    labels = []\n",
    "    for filename in filenames:\n",
    "        label = filename[:-2] # remove the \"-0\" suffix\n",
    "        label = label_to_integers(label)\n",
    "        labels.append(label)\n",
    "\n",
    "    return torch.tensor(labels, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import cache\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None, is_train=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = paths_to_labels(self.image_paths)\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    @cache # Don't cache if the transforms are not deterministic\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx].to(device)\n",
    "\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = torchvision.io.read_image(img_path, mode=ImageReadMode.GRAY)\n",
    "        # image = torchvision.io.read_image(\"train/lpbf-0.png\", mode=ImageReadMode.GRAY)\n",
    "        image = image.to(device).float()\n",
    "\n",
    "        # Invert image, making background pixels 0-intensity\n",
    "        image = 255.0 - image\n",
    "\n",
    "        # Apply normalization\n",
    "        image /= 255.0 # Scale from [0,255] to [0,1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset for loading CAPTCHA images\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "# Define the mean and standard deviation used for normalization\n",
    "RGB_MEAN = [0.485, 0.456, 0.406]\n",
    "RGB_STD = [0.229, 0.224, 0.225]\n",
    "GRAYSCALE_MEAN = [0.5]\n",
    "GRAYSCALE_STD = [0.5]\n",
    "NORM_MEAN = GRAYSCALE_MEAN # Modify this\n",
    "NORM_STD = GRAYSCALE_STD # Modify this\n",
    "\n",
    "TRAIN_VAL_SPLIT = [0.9, 0.1]\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Resize all images to this size\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "\n",
    "# Image transformations\n",
    "transform = v2.Compose([\n",
    "    v2.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    # v2.ColorJitter((1.0,1.0), (2.0,2.0)),\n",
    "    # v2.RandomAffine(degrees=30, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    # v2.RandomRotation(45),\n",
    "    # v2.Grayscale(),\n",
    "    v2.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "])\n",
    "\n",
    "image_folders = [\"train/\"]\n",
    "image_paths = []\n",
    "for image_folder in image_folders:\n",
    "    image_paths.extend(get_image_paths(image_folder))\n",
    "random.seed(RANDOM_SEED)\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "N_train = round(TRAIN_VAL_SPLIT[0] * len(image_paths))\n",
    "train_paths, val_paths = image_paths[:N_train], image_paths[N_train:]\n",
    "\n",
    "train_dataset = ImageDataset(train_paths, transform=transform, is_train=True)\n",
    "val_dataset = ImageDataset(val_paths, transform=transform, is_train=False)\n",
    "test_dataset = ImageDataset(get_image_paths(\"test/\"), transform=transform, is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Un-normalize the image\n",
    "def unnormalize(image, mean, std):\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "    std = torch.tensor(std).view(-1, 1, 1)\n",
    "    # image = image * std + mean  # Reverse normalization\n",
    "    return image * 255.0\n",
    "\n",
    "\n",
    "def display(\n",
    "    image: torch.Tensor,\n",
    "    actual_label: Optional[torch.Tensor] = None,\n",
    "    predicted_label: Optional[torch.Tensor] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Display a sample image\n",
    "    \"\"\"\n",
    "    image = unnormalize(image, NORM_MEAN, NORM_STD)\n",
    "    plt.figure(figsize=(12,24), facecolor='black')\n",
    "\n",
    "    if actual_label is not None:\n",
    "        actual_label = ''.join(VOCAB[i] for i in list(actual_label.int()) if i != PADDING_VALUE)\n",
    "    if predicted_label is not None:\n",
    "        predicted_label = ''.join(VOCAB[i] for i in list(predicted_label.int()) if i != PADDING_VALUE)\n",
    "\n",
    "    print(f\"Predicted: {predicted_label}, Actual: {actual_label}\")\n",
    "\n",
    "    for i in range(len(image)):\n",
    "        if torch.all(image[i] == 0):\n",
    "            continue\n",
    "        plt.subplot(1, 15, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image[i].squeeze(0).cpu().numpy().astype(np.uint8), cmap='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "images, labels = next(iter(train_dataloader))\n",
    "display(images[0], actual_label=labels[0]) # extract first sample from batch\n",
    "\n",
    "input_channels = images.shape[2] # Get the no. of channels in the image\n",
    "print(\"Input channels:\", input_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels, seq_length):  # 26 letters + 10 digits = 36 classes\n",
    "        super().__init__()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # CNN layers\n",
    "        first_layer_channels = 32\n",
    "        n_cnn_layers = 4\n",
    "        in_channels = input_channels\n",
    "        out_channels = None\n",
    "        cnn_layers = [\n",
    "            # nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding='same'),\n",
    "            # nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding='same'),\n",
    "            # nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding='same'),\n",
    "        ]\n",
    "        for i in range(1, n_cnn_layers+1):\n",
    "            out_channels = first_layer_channels if i == 1 else in_channels * 2\n",
    "            cnn_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size= 3 if i == 1 else 5, stride=1, padding='same')),\n",
    "            cnn_layers.append(nn.MaxPool2d(kernel_size=2, stride=2)),\n",
    "            cnn_layers.append(nn.ReLU()),\n",
    "            cnn_layers.append(nn.BatchNorm2d(out_channels)),\n",
    "            cnn_layers.append(nn.Dropout2d(p=0.30)),\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            *cnn_layers,\n",
    "            # nn.Dropout2d(p=0.25),\n",
    "        )\n",
    "\n",
    "        num_features = 4096\n",
    "        self.fc = nn.Sequential(\n",
    "            # nn.Dropout(p=0.25),\n",
    "            nn.Linear(num_features, num_features//2),\n",
    "            nn.Linear(num_features//2, num_features//4),\n",
    "            nn.Linear(num_features//4, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.cnn(x)\n",
    "        N, C, h, w = x.size()\n",
    "        x = x.view(N, -1) # flatten the matrix\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_loss(criterion, logits: torch.Tensor, labels: torch.Tensor):\n",
    "    return criterion(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, criterion, optimizer, val_dataloader=None, epochs=10):\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_samples = 0\n",
    "        for images, labels in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits: torch.Tensor = model(images)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = compute_loss(criterion, logits, labels)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # the criterion computes the mean across the batch by default\n",
    "            # the last batch might not have the same batch size as the other batches\n",
    "            # this way of loss calculation is more accurate\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            num_samples += images.size(0) # no. of samples in current batch\n",
    "\n",
    "        train_loss = total_loss / num_samples\n",
    "\n",
    "        val_loss = val_acc = None\n",
    "        if val_dataloader is not None:\n",
    "            val_loss, val_acc = test(model, val_dataloader, criterion=criterion)\n",
    "\n",
    "        print(f\"epoch [{epoch+1}/{epochs}], train_loss: {train_loss:.2f}, val_loss: {val_loss:.2f}, val_acc: {val_acc:.2f}\")\n",
    "        if val_loss is not None and val_loss <= best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"model.pth\")\n",
    "            print(\"Saved model state dict to model.pth\")\n",
    "\n",
    "\n",
    "def test(model, dataloader, criterion=None) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            logits: torch.Tensor = model(images)\n",
    "\n",
    "            # Compute loss\n",
    "            if criterion:\n",
    "                loss = compute_loss(criterion, logits, labels)\n",
    "                total_loss += loss.item() * images.size(0)\n",
    "\n",
    "            y_pred = logits.argmax(dim=1)\n",
    "\n",
    "            # Compare predictions with true labels\n",
    "            correct_predictions: torch.Tensor = (y_pred == labels)\n",
    "            num_correct += correct_predictions.sum()\n",
    "\n",
    "            num_samples += images.size(0)\n",
    "\n",
    "    if criterion:\n",
    "        return total_loss / num_samples, num_correct / num_samples\n",
    "\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training block\n",
    "model = ImageClassificationModel(num_classes=NUM_CLASSES, input_channels=input_channels, seq_length=MAX_LABEL_LEN).to(device)\n",
    "\n",
    "# Resume from checkpoint. Comment out to start training from scratch\n",
    "# state_dict = torch.load(\"model.pth\", map_location=device)\n",
    "# model.load_state_dict(state_dict)\n",
    "# # print(model)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PADDING_VALUE).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, amsgrad=True)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_dataloader, criterion, optimizer, val_dataloader=val_dataloader, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing block\n",
    "model = ImageClassificationModel(num_classes=NUM_CLASSES, input_channels=input_channels, seq_length=MAX_LABEL_LEN)\n",
    "model = model.to(device)\n",
    "state_dict = torch.load(\"model.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "train_acc = test(model, train_dataloader)\n",
    "print(f'train_acc: {train_acc * 100:.2f}%')\n",
    "val_acc = test(model, val_dataloader)\n",
    "print(f'val_acc: {val_acc * 100:.2f}%')\n",
    "test_acc = test(model, test_dataloader)\n",
    "print(f'test_acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model playground\n",
    "model.eval()\n",
    "for images, labels in test_dataloader:\n",
    "    # Forward pass\n",
    "    logits: torch.Tensor = model(images)\n",
    "\n",
    "    y_pred = logits.argmax(dim=1)\n",
    "    display(images[0], actual_label=labels[0], predicted_label=y_pred[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(\"model.pth\", map_location='cuda')\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
